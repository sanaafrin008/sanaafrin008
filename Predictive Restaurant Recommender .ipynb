{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjL9heaqt7FwQC/IGeKkC2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaafrin008/sanaafrin008/blob/main/Predictive%20Restaurant%20Recommender%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Predictive Restaurant Recommender (Colab)\n",
        "# End-to-end: data prep → features → models → submission\n",
        "# ===========================================\n",
        "\n",
        "import os, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------------------------------------------\n",
        "# 0) CONFIG\n",
        "# -------------------------------------------\n",
        "RANDOM_STATE = 42\n",
        "NEG_PER_POS = 5         # how many negative vendors per (customer,location)\n",
        "TOP_N_VENDORS = None    # None = use all vendors; or set (e.g., 300) to limit for speed\n",
        "THRESHOLD = 0.5         # probability threshold to output 0/1\n",
        "\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) LOAD DATA\n",
        "# -------------------------------------------\n",
        "def exists(fname): return os.path.exists(fname)\n",
        "\n",
        "required = [\n",
        "    \"orders.csv\", \"train_customers.csv\", \"train_locations.csv\",\n",
        "    \"vendors.csv\", \"test_customers.csv\", \"test_locations.csv\"\n",
        "]\n",
        "for f in required:\n",
        "    print((\"✅\" if exists(f) else \"❌\"), f)\n",
        "\n",
        "orders          = pd.read_csv(\"orders.csv\", low_memory=False)\n",
        "train_customers = pd.read_csv(\"train_customers.csv\")\n",
        "train_locations = pd.read_csv(\"train_locations.csv\")\n",
        "vendors         = pd.read_csv(\"vendors.csv\")\n",
        "test_customers  = pd.read_csv(\"test_customers.csv\")\n",
        "test_locations  = pd.read_csv(\"test_locations.csv\")\n",
        "\n",
        "sample_sub = None\n",
        "if exists(\"SampleSubmission.csv\"):\n",
        "    try:\n",
        "        sample_sub = pd.read_csv(\"SampleSubmission.csv\")\n",
        "        print(\"✅ Loaded SampleSubmission.csv (will drive exact test rows).\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Could not read SampleSubmission.csv:\", e)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) NORMALIZE COLUMN NAMES & KEYS\n",
        "# -------------------------------------------\n",
        "# orders has LOCATION_NUMBER (upper); vendors use id for vendor_id; orders may have LOCATION_TYPE (upper)\n",
        "orders = orders.rename(columns={\n",
        "    \"LOCATION_NUMBER\":\"location_number\",\n",
        "    \"LOCATION_TYPE\":\"location_type\"\n",
        "})\n",
        "vendors = vendors.rename(columns={\"id\":\"vendor_id\"})\n",
        "\n",
        "# Make sure location key exists in train_locations\n",
        "# Expected columns: ['customer_id', 'location_number', 'location_type', 'latitude', 'longitude']\n",
        "print(\"Train Locations columns:\", train_locations.columns.tolist())\n",
        "print(\"Test Locations columns:\", test_locations.columns.tolist())\n",
        "\n",
        "# Ensure numeric where expected\n",
        "for df, cols in [(orders, [\"item_count\",\"grand_total\",\"vendor_discount_amount\",\n",
        "                           \"deliverydistance\",\"preparationtime\",\"delivery_time\"]),\n",
        "                 (train_locations, [\"location_number\",\"latitude\",\"longitude\"]),\n",
        "                 (test_locations, [\"location_number\",\"latitude\",\"longitude\"]),\n",
        "                 (vendors, [\"vendor_id\",\"latitude\",\"longitude\",\"delivery_charge\",\"serving_distance\",\"commission\",\"rank\",\"vendor_rating\"])]:\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) POSITIVES: unique (customer, location_number, vendor_id) from orders\n",
        "# -------------------------------------------\n",
        "# If orders lacks location_number for some rows, try to fill from 'CID X LOC_NUM X VENDOR'\n",
        "if \"location_number\" not in orders.columns or orders[\"location_number\"].isna().all():\n",
        "    if \"CID X LOC_NUM X VENDOR\" in orders.columns:\n",
        "        # parse location number from composite key \"... X loc X vendor\"\n",
        "        def parse_locnum(s):\n",
        "            try:\n",
        "                parts = str(s).split(\" X \")\n",
        "                return float(parts[1])\n",
        "            except:\n",
        "                return np.nan\n",
        "        orders[\"location_number\"] = orders[\"CID X LOC_NUM X VENDOR\"].map(parse_locnum)\n",
        "    else:\n",
        "        raise ValueError(\"orders.csv lacks location_number and composite key; cannot build positives.\")\n",
        "\n",
        "pos = orders.loc[:, [\"customer_id\",\"location_number\",\"vendor_id\"]].dropna()\n",
        "pos[\"target\"] = 1\n",
        "pos = pos.drop_duplicates()\n",
        "\n",
        "# Optionally limit #vendors (speed)\n",
        "if TOP_N_VENDORS:\n",
        "    vendor_counts = orders[\"vendor_id\"].value_counts().index[:TOP_N_VENDORS]\n",
        "    pos = pos[pos[\"vendor_id\"].isin(vendor_counts)]\n",
        "\n",
        "print(\"Positives:\", pos.shape)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) NEGATIVES: sample vendors the customer-location never ordered from\n",
        "# -------------------------------------------\n",
        "# Build map: for each (customer,location), which vendors were ordered\n",
        "pos_key = set(zip(pos.customer_id, pos.location_number, pos.vendor_id))\n",
        "active_vendors = sorted(pos[\"vendor_id\"].dropna().unique().tolist())\n",
        "\n",
        "# group locations per customer from TRAIN locations (history side)\n",
        "cust_locs = train_locations.groupby(\"customer_id\")[\"location_number\"].unique()\n",
        "\n",
        "neg_rows = []\n",
        "for cid, loc_list in cust_locs.items():\n",
        "    for loc in loc_list:\n",
        "        # pick negative vendors (not ordered for this cid,loc)\n",
        "        # speed: sample without replacement from active_vendors\n",
        "        tried = 0\n",
        "        sampled = 0\n",
        "        # build a small random subset for speed\n",
        "        vendor_pool = np.random.permutation(active_vendors)\n",
        "        for vid in vendor_pool:\n",
        "            tried += 1\n",
        "            if (cid, loc, vid) not in pos_key:\n",
        "                neg_rows.append((cid, loc, vid, 0))\n",
        "                sampled += 1\n",
        "                if sampled >= NEG_PER_POS:\n",
        "                    break\n",
        "\n",
        "neg = pd.DataFrame(neg_rows, columns=[\"customer_id\",\"location_number\",\"vendor_id\",\"target\"])\n",
        "print(\"Negatives (sampled):\", neg.shape)\n",
        "\n",
        "# Combine\n",
        "train_pairs = pd.concat([pos, neg], ignore_index=True)\n",
        "print(\"Train pairs (pos+neg):\", train_pairs.shape)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 5) FEATURE ENGINEERING\n",
        "# -------------------------------------------\n",
        "# 5a) Aggregates from orders (history)\n",
        "def safe_count(df, by_cols, name):\n",
        "    g = orders.groupby(by_cols).size().reset_index(name=name)\n",
        "    return g\n",
        "\n",
        "# customer-level\n",
        "f_cust_cnt   = safe_count(orders, [\"customer_id\"], \"f_cust_orders\")\n",
        "# vendor-level\n",
        "f_ven_cnt    = safe_count(orders, [\"vendor_id\"], \"f_vendor_orders\")\n",
        "# customer-vendor history (how many times this c ordered from this v)\n",
        "f_cv_cnt     = safe_count(orders, [\"customer_id\",\"vendor_id\"], \"f_cv_orders\")\n",
        "# location-vendor popularity (how often this vendor was used at this location overall)\n",
        "f_lv_cnt     = safe_count(orders, [\"location_number\",\"vendor_id\"], \"f_lv_orders\")\n",
        "# customer-location activity\n",
        "f_cl_cnt     = safe_count(orders, [\"customer_id\",\"location_number\"], \"f_cl_orders\")\n",
        "\n",
        "# 5b) geo distance (customer location ↔ vendor)\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
        "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return 6371.0 * c  # km\n",
        "\n",
        "# 5c) Merge cores: customers (demographics), locations (geo), vendors (meta)\n",
        "# Vendors already renamed to vendor_id\n",
        "vendors_slim = vendors.copy()\n",
        "# pick a useful subset of vendor features (add more if you like)\n",
        "keep_vendor_cols = [c for c in [\"vendor_id\",\"latitude\",\"longitude\",\"vendor_category_en\",\"delivery_charge\",\n",
        "                                \"serving_distance\",\"commission\",\"rank\",\"vendor_rating\",\"status\",\"verified\",\n",
        "                                \"language\"] if c in vendors_slim.columns]\n",
        "vendors_slim = vendors_slim[keep_vendor_cols].drop_duplicates(\"vendor_id\")\n",
        "\n",
        "# Prepare train feature table\n",
        "feat = train_pairs.merge(train_customers, on=\"customer_id\", how=\"left\") \\\n",
        "                  .merge(train_locations, on=[\"customer_id\",\"location_number\"], how=\"left\") \\\n",
        "                  .merge(vendors_slim, on=\"vendor_id\", how=\"left\")\n",
        "\n",
        "# rename to avoid col overlaps\n",
        "feat = feat.rename(columns={\n",
        "    \"latitude_x\":\"cust_lat\", \"longitude_x\":\"cust_lon\",\n",
        "    \"latitude_y\":\"vend_lat\", \"longitude_y\":\"vend_lon\"\n",
        "} if \"latitude_x\" in feat.columns else {})\n",
        "\n",
        "# If there wasn't a collision, ensure columns exist\n",
        "if \"cust_lat\" not in feat.columns and \"latitude\" in train_locations.columns:\n",
        "    feat = feat.rename(columns={\"latitude\":\"cust_lat\",\"longitude\":\"cust_lon\"})\n",
        "if \"vend_lat\" not in feat.columns and \"latitude\" in vendors_slim.columns:\n",
        "    feat = feat.rename(columns={\"latitude\":\"vend_lat\",\"longitude\":\"vend_lon\"})\n",
        "\n",
        "# compute distance\n",
        "feat[\"dist_km\"] = haversine(feat[\"cust_lat\"], feat[\"cust_lon\"], feat[\"vend_lat\"], feat[\"vend_lon\"])\n",
        "\n",
        "# add aggregates\n",
        "for gdf in [f_cust_cnt, f_ven_cnt, f_cv_cnt, f_lv_cnt, f_cl_cnt]:\n",
        "    feat = feat.merge(gdf, on=[c for c in gdf.columns if c.startswith((\"customer_id\",\"vendor_id\",\"location_number\"))], how=\"left\")\n",
        "\n",
        "# fill NA numeric\n",
        "for c in feat.columns:\n",
        "    if pd.api.types.is_numeric_dtype(feat[c]):\n",
        "        feat[c] = feat[c].fillna(0)\n",
        "\n",
        "# encode categoricals with simple mapping (train-time fit)\n",
        "def fit_map(series):\n",
        "    vals = series.astype(str).fillna(\"UNK\").unique().tolist()\n",
        "    return {v:i for i,v in enumerate(vals)}\n",
        "\n",
        "cat_cols = []\n",
        "for c in [\"gender\",\"status\",\"language\",\"location_type\",\"vendor_category_en\",\"verified\",\"status\"]:\n",
        "    if c in feat.columns and feat[c].dtype == object:\n",
        "        cat_cols.append(c)\n",
        "\n",
        "encoders = {}\n",
        "for c in cat_cols:\n",
        "    encoders[c] = fit_map(feat[c])\n",
        "    feat[c] = feat[c].astype(str).map(encoders[c]).fillna(-1).astype(int)\n",
        "\n",
        "# target\n",
        "y = feat[\"target\"].astype(int)\n",
        "\n",
        "# select features (drop IDs/text/date-ish)\n",
        "drop_cols = set([\"target\",\"customer_id\",\"vendor_id\",\"location_number\",\"dob\",\"created_at\",\"updated_at\"])\n",
        "X = feat.drop(columns=[c for c in drop_cols if c in feat.columns], errors=\"ignore\")\n",
        "\n",
        "# Keep only numeric columns\n",
        "X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "# -------------------------------------------\n",
        "# 6) TRAIN/VAL SPLIT + MODELS\n",
        "# -------------------------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# Baseline: RandomForest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300, max_depth=None, n_jobs=-1, random_state=RANDOM_STATE\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_val_prob = rf.predict_proba(X_val)[:,1]\n",
        "rf_val_pred = (rf_val_prob >= THRESHOLD).astype(int)\n",
        "\n",
        "print(\"RandomForest  AUC:\", round(roc_auc_score(y_val, rf_val_prob), 4))\n",
        "print(\"RandomForest  ACC:\", round(accuracy_score(y_val, rf_val_pred), 4))\n",
        "\n",
        "# Stronger: LightGBM (optional)\n",
        "use_lgbm = False\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    use_lgbm = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if use_lgbm:\n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "    dval   = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
        "    params = dict(\n",
        "        objective=\"binary\", metric=\"auc\", boosting_type=\"gbdt\",\n",
        "        num_leaves=64, learning_rate=0.05,\n",
        "        feature_fraction=0.85, bagging_fraction=0.8, bagging_freq=5,\n",
        "        verbosity=-1, seed=RANDOM_STATE\n",
        "    )\n",
        "    lgbm = lgb.train(\n",
        "        params, dtrain, valid_sets=[dtrain, dval],\n",
        "        num_boost_round=2000\n",
        "    )\n",
        "    lgb_val_prob = lgbm.predict(X_val, num_iteration=lgbm.best_iteration)\n",
        "    lgb_val_pred = (lgb_val_prob >= THRESHOLD).astype(int)\n",
        "    print(\"LightGBM      AUC:\", round(roc_auc_score(y_val, lgb_val_prob), 4))\n",
        "    print(\"LightGBM      ACC:\", round(accuracy_score(y_val, lgb_val_pred), 4))\n",
        "\n",
        "# choose best model for inference\n",
        "best_model_name = \"lgbm\" if use_lgbm else \"rf\"\n",
        "best_model = lgbm if use_lgbm else rf\n",
        "print(\"Using model:\", best_model_name)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 7) BUILD TEST CANDIDATES (CID × LOC_NUM × VENDOR)\n",
        "#    Prefer SampleSubmission.csv to match exactly.\n",
        "# -------------------------------------------\n",
        "if sample_sub is not None and \"CID X LOC_NUM X VENDOR\" in sample_sub.columns:\n",
        "    # Use exactly the keys they expect\n",
        "    keys = sample_sub[\"CID X LOC_NUM X VENDOR\"].astype(str)\n",
        "\n",
        "    def parse_key(s):\n",
        "        parts = str(s).split(\" X \")\n",
        "        return parts[0], float(parts[1]), int(parts[2])\n",
        "    parsed = keys.map(parse_key)\n",
        "    test_pairs = pd.DataFrame(parsed.tolist(), columns=[\"customer_id\",\"location_number\",\"vendor_id\"])\n",
        "else:\n",
        "    # Generate all combinations of test customers × their locations × vendors seen in training\n",
        "    test_pairs = (\n",
        "        test_locations[[\"customer_id\",\"location_number\",\"location_type\",\"latitude\",\"longitude\"]]\n",
        "        .merge(test_customers, on=\"customer_id\", how=\"left\")\n",
        "    )[[\"customer_id\",\"location_number\"]] \\\n",
        "    .drop_duplicates()\n",
        "\n",
        "    vendor_list = active_vendors if TOP_N_VENDORS is None else active_vendors[:TOP_N_VENDORS]\n",
        "    test_pairs = test_pairs.assign(_key=1).merge(\n",
        "        pd.DataFrame({\"vendor_id\": vendor_list, \"_key\":[1]*len(vendor_list)}),\n",
        "        on=\"_key\", how=\"left\"\n",
        "    ).drop(columns=\"_key\")\n",
        "\n",
        "print(\"Test candidate pairs:\", test_pairs.shape)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 8) BUILD TEST FEATURES (mirror training)\n",
        "# -------------------------------------------\n",
        "# Ensure test_locations is unique on the merge key before merging\n",
        "test_locations_unique = test_locations.drop_duplicates(subset=[\"customer_id\", \"location_number\"])\n",
        "\n",
        "test_feat = test_pairs.merge(test_customers, on=\"customer_id\", how=\"left\")\n",
        "print(\"Shape after merging test_pairs with test_customers:\", test_feat.shape)\n",
        "\n",
        "test_feat = test_feat.merge(test_locations_unique, on=[\"customer_id\",\"location_number\"], how=\"left\")\n",
        "print(\"Shape after merging with test_locations_unique:\", test_feat.shape)\n",
        "\n",
        "\n",
        "test_feat = test_feat.merge(vendors_slim, on=\"vendor_id\", how=\"left\")\n",
        "print(\"Shape after merging with vendors_slim:\", test_feat.shape)\n",
        "\n",
        "\n",
        "# Align location/vendor lat/lon naming then compute distance\n",
        "if \"latitude_x\" in test_feat.columns:\n",
        "    test_feat = test_feat.rename(columns={\n",
        "        \"latitude_x\":\"cust_lat\",\"longitude_x\":\"cust_lon\",\n",
        "        \"latitude_y\":\"vend_lat\",\"longitude_y\":\"vend_lon\"\n",
        "    })\n",
        "else:\n",
        "    if \"latitude\" in test_locations_unique.columns:\n",
        "        test_feat = test_feat.rename(columns={\"latitude\":\"cust_lat\",\"longitude\":\"cust_lon\"})\n",
        "    if \"latitude\" in vendors_slim.columns:\n",
        "        test_feat = test_feat.rename(columns={\"latitude\":\"vend_lat\",\"longitude\":\"vend_lon\"})\n",
        "\n",
        "test_feat[\"dist_km\"] = haversine(test_feat[\"cust_lat\"], test_feat[\"cust_lon\"], test_feat[\"vend_lat\"], test_feat[\"vend_lon\"])\n",
        "\n",
        "# merge aggregates\n",
        "for gdf in [f_cust_cnt, f_ven_cnt, f_cv_cnt, f_lv_cnt, f_cl_cnt]:\n",
        "    test_feat = test_feat.merge(gdf, on=[c for c in gdf.columns if c.startswith((\"customer_id\",\"vendor_id\",\"location_number\"))], how=\"left\")\n",
        "\n",
        "# fill numeric NA\n",
        "for c in test_feat.columns:\n",
        "    if pd.api.types.is_numeric_dtype(test_feat[c]):\n",
        "        test_feat[c] = test_feat[c].fillna(0)\n",
        "\n",
        "\n",
        "# apply categorical encoders (same maps as train; unseen -> -1)\n",
        "for c in cat_cols:\n",
        "    if c in test_feat.columns:\n",
        "        test_feat[c] = test_feat[c].astype(str).map(encoders[c]).fillna(-1).astype(int)\n",
        "\n",
        "# select & order columns as X had\n",
        "X_cols = X.columns.tolist()\n",
        "X_test = test_feat.reindex(columns=X_cols, fill_value=0)\n",
        "\n",
        "# Ensure same order and unique keys for X_test as test_pairs\n",
        "# This aligns the test features with the submission structure to prevent the ValueError\n",
        "X_test = X_test.loc[test_pairs.index]\n",
        "\n",
        "# -------------------------------------------\n",
        "# 9) PREDICT ON TEST\n",
        "# -------------------------------------------\n",
        "if best_model_name == \"lgbm\":\n",
        "    test_prob = best_model.predict(X_test, num_iteration=best_model.best_iteration)\n",
        "else:\n",
        "    test_prob = best_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "test_pred = (test_prob >= THRESHOLD).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxqvEzMaJjrJ",
        "outputId": "c926188c-d992-4ae1-dd7a-ae35e4490b75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ orders.csv\n",
            "✅ train_customers.csv\n",
            "✅ train_locations.csv\n",
            "✅ vendors.csv\n",
            "✅ test_customers.csv\n",
            "✅ test_locations.csv\n",
            "Train Locations columns: ['customer_id', 'location_number', 'location_type', 'latitude', 'longitude']\n",
            "Test Locations columns: ['customer_id', 'location_number', 'location_type', 'latitude', 'longitude']\n",
            "Positives: (80142, 4)\n",
            "Negatives (sampled): (297515, 4)\n",
            "Train pairs (pos+neg): (377657, 4)\n",
            "RandomForest  AUC: 0.9987\n",
            "RandomForest  ACC: 0.9909\n",
            "LightGBM      AUC: 0.9988\n",
            "LightGBM      ACC: 0.9914\n",
            "Using model: lgbm\n",
            "Test candidate pairs: (1672000, 3)\n",
            "Shape after merging test_pairs with test_customers: (1673600, 10)\n",
            "Shape after merging with test_locations_unique: (1673600, 13)\n",
            "Shape after merging with vendors_slim: (1673600, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sub), len(test_pred))\n"
      ],
      "metadata": {
        "id": "3A8Do9TfZLev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Create and train model using the correct variable names\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set using the correct variable name\n",
        "test_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "Ct0fpqG3Zlrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brFIUxQPZwZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "oCRaflwMZO4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub[\"target\"] = test_pred[:len(sub)]\n"
      ],
      "metadata": {
        "id": "P6HL8ZHdZSyo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = test_pred[:len(sub)]  # trim extra\n",
        "sub[\"target\"] = test_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "BIgOAMHMZVXZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"len(X_test):\", len(X_test))\n",
        "print(\"len(test_pred):\", len(test_pred))\n",
        "print(\"len(sub):\", len(sub))\n",
        "# Removed print(\"len(test):\", len(test)) as 'test' is not defined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfNNAKRBaLmE",
        "outputId": "5691fd0d-5ccd-4b3b-bf65-347f7a417ebb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(X_test): 1673600\n",
            "len(test_pred): 1672000\n",
            "len(sub): 1672000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume sub and X_test are already defined\n",
        "\n",
        "print(\"Unique IDs in sub:\", sub[\"CID X LOC_NUM X VENDOR\"].nunique())\n",
        "print(\"Rows in sub:\", len(sub))\n",
        "print(\"Rows in X_test:\", len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD_xaVotaiX_",
        "outputId": "bb500b16-4038-4b6f-8ca7-acaef6fa50a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique IDs in sub: 1672000\n",
            "Rows in sub: 1672000\n",
            "Rows in X_test: 1672000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub[\"target\"] = test_pred\n"
      ],
      "metadata": {
        "id": "PtVd8hKXay5H"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = sub[[\"CID X LOC_NUM X VENDOR\", \"target\"]].copy()\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"✅ Submission file created: submission.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsL_F3GOa45C",
        "outputId": "42d0f597-29af-448d-dd16-d949fa4fe4fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Submission file created: submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission.head())\n",
        "print(\"Shape of submission:\", submission.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1iIrHpPa8z2",
        "outputId": "77e58593-1218-4229-e697-e1b77d954c41"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CID X LOC_NUM X VENDOR  target\n",
            "0        Z59FTQD X 0 X 4       0\n",
            "1       Z59FTQD X 0 X 13       0\n",
            "2       Z59FTQD X 0 X 20       0\n",
            "3       Z59FTQD X 0 X 23       0\n",
            "4       Z59FTQD X 0 X 28       0\n",
            "Shape of submission: (1672000, 2)\n"
          ]
        }
      ]
    }
  ]
}